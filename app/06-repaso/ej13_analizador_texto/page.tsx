import IntroPythonXom from '@/components/IntroPythonXom';

const analizadorTextoExercise = {
  id: 'ej13_analizador_texto',  title: 'Analizador de Texto - Procesamiento Avanzado de Cadenas',  
  description: `Desarrolla un analizador completo de texto que extrae estad√≠sticas, patrones y m√©tricas avanzadas. Aprender√°s procesamiento de strings, an√°lisis de frecuencias, expresiones regulares y t√©cnicas de procesamiento de lenguaje natural.
- Extracci√≥n de patrones (emails, URLs, tel√©fonos)

**Parte 3: M√©tricas de Complejidad**
- √çndice de diversidad l√©xica
- An√°lisis de sentimientos b√°sico
- Detecci√≥n de palabras clave

**Parte 4: Generaci√≥n de Reportes**
- Visualizaci√≥n de estad√≠sticas
- Comparaci√≥n entre textos
- Exportaci√≥n de resultados`,
  starterCode: `import re
import string
from collections import Counter
import math

def estadisticas_basicas(texto):
    """
    Calcula estad√≠sticas b√°sicas del texto
    """
    # TODO: Implementar conteo de caracteres, palabras, l√≠neas
    pass

def frecuencia_palabras(texto, top_n=10):
    """
    Calcula la frecuencia de palabras m√°s comunes
    """
    # TODO: Implementar an√°lisis de frecuencia
    pass

def frecuencia_caracteres(texto):
    """
    Analiza la frecuencia de cada car√°cter
    """
    # TODO: Implementar an√°lisis de caracteres
    pass

def extraer_patrones(texto):
    """
    Extrae patrones comunes: emails, URLs, tel√©fonos
    """
    # TODO: Implementar extracci√≥n con regex
    pass

def calcular_legibilidad(texto):
    """
    Calcula √≠ndices de legibilidad del texto
    """
    # TODO: Implementar m√©tricas de legibilidad
    pass

def diversidad_lexica(texto):
    """
    Calcula la diversidad del vocabulario usado
    """
    # TODO: Implementar an√°lisis de diversidad
    pass

def generar_reporte(texto):
    """
    Genera un reporte completo del an√°lisis
    """
    # TODO: Integrar todos los an√°lisis
    pass`,
  tests: [
    {
      input: "Hola mundo. Este es un texto de prueba para analizar.",
      expected: "{'caracteres': 55, 'palabras': 11, 'oraciones': 2}",
      description: "Estad√≠sticas b√°sicas de texto simple"
    },
    {
      input: "Python es genial! Python es poderoso. Python es vers√°til.",
      expected: "{'python': 3, 'es': 3}",
      description: "Frecuencia de palabras repetidas"
    },
    {
      input: "Contacto: juan@email.com o https://website.com",
      expected: "{'emails': ['juan@email.com'], 'urls': ['https://website.com']}",
      description: "Extracci√≥n de patrones"
    },
    {
      input: "El gato subi√≥ al tejado. El perro ladr√≥ fuerte.",
      expected: "TTR > 0.7",
      description: "Diversidad l√©xica alta"
    },
    {
      input: "a a a a a a a a a a",
      expected: "TTR < 0.2",
      description: "Diversidad l√©xica baja"
    },
    {
      input: "",
      expected: "{'caracteres': 0, 'palabras': 0, 'oraciones': 0}",
      description: "Texto vac√≠o"
    },
    {
      input: "¬°Qu√© d√≠a tan maravilloso! ¬øC√≥mo est√°s?",
      expected: "Puntuaci√≥n especial detectada",
      description: "Texto con caracteres especiales"
    }
  ],
  hints: [
    {
      id: "division-palabras",
      text: "Usa split() para dividir en palabras, pero considera tambi√©n re.findall(r'\\b\\w+\\b', texto) para mayor precisi√≥n",
      type: "implementation"
    },
    {
      id: "limpieza-texto",
      text: "Para an√°lisis de frecuencia, convierte a min√∫sculas y elimina puntuaci√≥n con texto.translate(str.maketrans('', '', string.punctuation))",
      type: "preprocessing"
    },
    {
      id: "regex-patrones",
      text: "Patrones √∫tiles: email r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', URL r'https?://\\S+'",
      type: "syntax"
    },
    {
      id: "diversidad-ttr",
      text: "TTR (Type-Token Ratio) = tipos √∫nicos / total de tokens. Valores cercanos a 1 indican alta diversidad",
      type: "concept"
    }
  ],
  modelSolution: {
    code: `import re
import string
from collections import Counter
import math

def estadisticas_basicas(texto):
    """
    Calcula estad√≠sticas b√°sicas del texto
    """
    if not texto:
        return {
            'caracteres': 0,
            'caracteres_sin_espacios': 0,
            'palabras': 0,
            'oraciones': 0,
            'parrafos': 0,
            'lineas': 0
        }
    
    # Conteos b√°sicos
    caracteres = len(texto)
    caracteres_sin_espacios = len(texto.replace(' ', ''))
    palabras = len(re.findall(r'\\b\\w+\\b', texto))
    oraciones = len(re.findall(r'[.!?]+', texto))
    parrafos = len([p for p in texto.split('\\n\\n') if p.strip()])
    lineas = len(texto.split('\\n'))
    
    return {
        'caracteres': caracteres,
        'caracteres_sin_espacios': caracteres_sin_espacios,
        'palabras': palabras,
        'oraciones': oraciones,
        'parrafos': parrafos,
        'lineas': lineas,
        'promedio_palabras_por_oracion': round(palabras / oraciones, 2) if oraciones > 0 else 0,
        'promedio_caracteres_por_palabra': round(caracteres_sin_espacios / palabras, 2) if palabras > 0 else 0
    }

def frecuencia_palabras(texto, top_n=10):
    """
    Calcula la frecuencia de palabras m√°s comunes
    """
    # Limpiar y normalizar texto
    texto_limpio = texto.lower().translate(str.maketrans('', '', string.punctuation))
    palabras = re.findall(r'\\b\\w+\\b', texto_limpio)
    
    if not palabras:
        return {}
    
    # Calcular frecuencias
    frecuencias = Counter(palabras)
    
    # Retornar top N palabras
    return dict(frecuencias.most_common(top_n))

def frecuencia_caracteres(texto):
    """
    Analiza la frecuencia de cada car√°cter
    """
    if not texto:
        return {}
    
    # Contar todos los caracteres
    frecuencias = Counter(texto.lower())
    
    # Separar por categor√≠as
    letras = {k: v for k, v in frecuencias.items() if k.isalpha()}
    numeros = {k: v for k, v in frecuencias.items() if k.isdigit()}
    espacios = {k: v for k, v in frecuencias.items() if k.isspace()}
    puntuacion = {k: v for k, v in frecuencias.items() if k in string.punctuation}
    
    return {
        'total': dict(frecuencias.most_common()),
        'letras': dict(sorted(letras.items(), key=lambda x: x[1], reverse=True)),
        'numeros': numeros,
        'espacios': espacios,
        'puntuacion': puntuacion
    }

def extraer_patrones(texto):
    """
    Extrae patrones comunes: emails, URLs, tel√©fonos
    """
    patrones = {
        'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',
        'urls': r'https?://\\S+|www\\.\\S+',
        'telefonos': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b|\\(\\d{3}\\)\\s*\\d{3}[-.]?\\d{4}',
        'fechas': r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b',
        'hashtags': r'#\\w+',
        'menciones': r'@\\w+'
    }
    
    resultados = {}
    for nombre, patron in patrones.items():
        coincidencias = re.findall(patron, texto, re.IGNORECASE)
        if coincidencias:
            resultados[nombre] = coincidencias
    
    return resultados

def calcular_legibilidad(texto):
    """
    Calcula √≠ndices de legibilidad del texto
    """
    stats = estadisticas_basicas(texto)
    
    if stats['oraciones'] == 0 or stats['palabras'] == 0:
        return {'flesch_reading_ease': 0, 'flesch_kincaid_grade': 0}
    
    # Contar s√≠labas (aproximaci√≥n simple)
    def contar_silabas(palabra):
        palabra = palabra.lower()
        silabas = len(re.findall(r'[aeiou√°√©√≠√≥√∫√º]', palabra))
        return max(1, silabas)  # M√≠nimo 1 s√≠laba por palabra
    
    palabras = re.findall(r'\\b\\w+\\b', texto.lower())
    total_silabas = sum(contar_silabas(palabra) for palabra in palabras)
    
    # √çndice Flesch Reading Ease (adaptado al espa√±ol)
    flesch_score = 206.835 - (1.015 * (stats['palabras'] / stats['oraciones'])) - (84.6 * (total_silabas / stats['palabras']))
    
    # Nivel Flesch-Kincaid
    fk_grade = (0.39 * (stats['palabras'] / stats['oraciones'])) + (11.8 * (total_silabas / stats['palabras'])) - 15.59
    
    return {
        'flesch_reading_ease': round(flesch_score, 2),
        'flesch_kincaid_grade': round(max(0, fk_grade), 2),
        'total_silabas': total_silabas,
        'interpretacion': interpretar_flesch(flesch_score)
    }

def interpretar_flesch(score):
    """
    Interpreta el puntaje Flesch
    """
    if score >= 90:
        return "Muy f√°cil"
    elif score >= 80:
        return "F√°cil"
    elif score >= 70:
        return "Bastante f√°cil"
    elif score >= 60:
        return "Normal"
    elif score >= 50:
        return "Bastante dif√≠cil"
    elif score >= 30:
        return "Dif√≠cil"
    else:
        return "Muy dif√≠cil"

def diversidad_lexica(texto):
    """
    Calcula la diversidad del vocabulario usado
    """
    palabras = re.findall(r'\\b\\w+\\b', texto.lower())
    
    if not palabras:
        return {'ttr': 0, 'tipos': 0, 'tokens': 0}
    
    tipos_unicos = len(set(palabras))
    total_tokens = len(palabras)
    ttr = tipos_unicos / total_tokens
    
    # √çndice de diversidad de Simpson
    frecuencias = Counter(palabras)
    simpson_index = sum((freq/total_tokens)**2 for freq in frecuencias.values())
    diversidad_simpson = 1 - simpson_index
    
    return {
        'ttr': round(ttr, 3),
        'tipos': tipos_unicos,
        'tokens': total_tokens,
        'diversidad_simpson': round(diversidad_simpson, 3),
        'interpretacion_ttr': interpretar_ttr(ttr)
    }

def interpretar_ttr(ttr):
    """
    Interpreta el valor TTR
    """
    if ttr >= 0.8:
        return "Muy alta diversidad"
    elif ttr >= 0.6:
        return "Alta diversidad"
    elif ttr >= 0.4:
        return "Diversidad media"
    elif ttr >= 0.2:
        return "Baja diversidad"
    else:
        return "Muy baja diversidad"

def generar_reporte(texto):
    """
    Genera un reporte completo del an√°lisis
    """
    if not texto.strip():
        return "El texto est√° vac√≠o o solo contiene espacios en blanco."
    
    print("=" * 60)
    print("üìä REPORTE DE AN√ÅLISIS DE TEXTO")
    print("=" * 60)
    
    # Estad√≠sticas b√°sicas
    stats = estadisticas_basicas(texto)
    print("\\nüìà ESTAD√çSTICAS B√ÅSICAS:")
    for clave, valor in stats.items():
        print(f"  ‚Ä¢ {clave.replace('_', ' ').title()}: {valor}")
    
    # Frecuencia de palabras
    print("\\nüî§ PALABRAS M√ÅS FRECUENTES:")
    freq_palabras = frecuencia_palabras(texto, 5)
    for palabra, freq in freq_palabras.items():
        print(f"  ‚Ä¢ '{palabra}': {freq} veces")
    
    # Patrones encontrados
    patrones = extraer_patrones(texto)
    if patrones:
        print("\\nüîç PATRONES ENCONTRADOS:")
        for tipo, lista in patrones.items():
            print(f"  ‚Ä¢ {tipo.title()}: {lista}")
    
    # Legibilidad
    legibilidad = calcular_legibilidad(texto)
    print("\\nüìñ AN√ÅLISIS DE LEGIBILIDAD:")
    print(f"  ‚Ä¢ √çndice Flesch: {legibilidad['flesch_reading_ease']} ({legibilidad['interpretacion']})")
    print(f"  ‚Ä¢ Nivel Flesch-Kincaid: {legibilidad['flesch_kincaid_grade']}")
    
    # Diversidad l√©xica
    diversidad = diversidad_lexica(texto)
    print("\\nüé® DIVERSIDAD L√âXICA:")
    print(f"  ‚Ä¢ TTR (Type-Token Ratio): {diversidad['ttr']} ({diversidad['interpretacion_ttr']})")
    print(f"  ‚Ä¢ Palabras √∫nicas: {diversidad['tipos']}")
    print(f"  ‚Ä¢ Total de palabras: {diversidad['tokens']}")
    
    return "An√°lisis completado exitosamente."

# Ejemplo de uso
if __name__ == "__main__":
    texto_ejemplo = """
    El an√°lisis de texto es una disciplina fascinante que combina ling√º√≠stica, 
    estad√≠stica y programaci√≥n. Python ofrece herramientas poderosas para 
    procesar y analizar grandes vol√∫menes de texto de manera eficiente.
    
    ¬øSab√≠as que puedes extraer patrones como emails (contacto@ejemplo.com) 
    o URLs (https://www.python.org) usando expresiones regulares?
    
    Este analizador puede procesar textos en espa√±ol y proporcionar m√©tricas 
    √∫tiles para escritores, investigadores y desarrolladores.
    """
    
    generar_reporte(texto_ejemplo)`,
    explanation: `**üîç Explicaci√≥n de la Soluci√≥n:**

1. **Estad√≠sticas B√°sicas**: Conteo completo de elementos textuales usando regex y m√©todos de string
2. **An√°lisis de Frecuencia**: Counter para eficiencia y normalizaci√≥n de texto para precisi√≥n
3. **Extracci√≥n de Patrones**: Regex especializadas para diferentes tipos de datos
4. **Legibilidad**: Implementaci√≥n de √≠ndices Flesch adaptados al espa√±ol
5. **Diversidad L√©xica**: TTR e √≠ndice de Simpson para medir riqueza vocabular

**üöÄ Conceptos Clave:**
- Procesamiento de lenguaje natural
- Expresiones regulares avanzadas
- M√©tricas de legibilidad
- An√°lisis estad√≠stico de texto
- Normalizaci√≥n y limpieza de datos`
  }
};

export default function Page() {
  return (
    <IntroPythonXom data={analizadorTextoExercise} />
  );
}
